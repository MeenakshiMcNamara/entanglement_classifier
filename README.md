# entanglement_classifier
The main purpose of this repo is to create a neural network classifier for determining ttbar production mode between gg, qqbar, and other. Below is a short description of the organizational scheme of the repo:


*****
The saved models after runs are in /models note that they are saved separately by the number of layers in the network, and that I created a folder in /two_layers for older versions that are no longer relevent. Optimizer states are also saved in /models... their names should make which ones they are clear.

The code which can be used to analyze the results and look for correlations is in /analysis_code. This includes confusion matrices, correlation matrices, and both the derivative and permutation methods of determining importance, as well as any other code used to analyze the data from runs. Results from analysis go into the /results subfolder. Try to keep these as organized as possible by making any relavent subfolders in it to group together information. Note that I have put copies of the files (including ones for running stuff in parallel) in a separate (less organized) subfolder. When running any of these, please ensure you are loading the correct validation data for the model to avoid producing artificially high accuracy in the analysis.

slurm submission scripts (used for submitting jobs to run on the clusters) are in /[insert numer]_layer_classifiers/slurm_submission_scripts. (At this point this only exists for three layers, but add this directory as you create and use other network archetectures). You can copy a script into the parent directory using `cp [insert submission script name].sub ..` and use a variant of the `sbatch  -A scholar --nodes=1 --gres=gpu:1 --gpus=1 -t 04:00:00 submissions.sub` command to run them individually (note that 4 hours is the max for scholar gpu sessions), or run `source submit_all.sh` (you will need make a version of this for future archetectures) to run all the versions you want at once (add/ comment out scripts to run as you like). To view your slurm jobs in any directory use `squeue -u <username>` and to cancel jobs use `scancel <jobid>`.

/code_to_import contains python files which are only used when imported in a variety of places. At the moment these are the dataset_preprocessing and Classifier_module. Note that to use these in a sister folder you should use `import sys` and `sys.path.append("<path to entanglement_classifier directory>")` and then prepend `code_to_import` to the file name in an import.

/slurm_output should contain all the outputs that you probably don't care about anymore, but want to keep just in case. You should also make one of these in each /[insert numer]_layer_classifiers directory, and move slurm output files into these directories after they have finished running. 

/data contains the numpy array data for the training loss and validation loss, as well as the validation and training data used for each model. Model names are included in the names of the saved files for easy identification.

/[insert number]_layer_classifiers contain the code which creates and runs the classifiers. Put older versions you want to save in subdirectories to keep things organized. Note that I prefer to keep a single jupyter notebook version which can be modified as I like, as well as a single .py version which takes specifications like channel type (ee, emu, or mumu) and cuts applied as command arguments which can be submitted on slurm. This makes it easy to tell which files do what. To run the .py script use `python [insert number]_layer_classifier.py --[arg1]=[input1] --[arg2]=[input2]...` after activating the environment (`source activate GANS` or some equivalent... it is recommended to add the loading of modules to your .bashrc or equivalent to save time). To create the .py file you can simply download the .ipynb file as a python file and then upload the python version. If any jupyter magic is used you must replace it with a python equivalent, but I think we have removed most of this by importing python files. This directory also contains submission scripts and subdirectories with slurm outputs and additional submission scripts.

/data_adders contains the code for generating the root files and data frames with the event data being used for training.

/useful_instructions contains documents with useful information and instructions for people new to the framework.
