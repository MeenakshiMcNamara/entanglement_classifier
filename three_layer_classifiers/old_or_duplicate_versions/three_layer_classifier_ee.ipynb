{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "######################################### Import statements ###########################################\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")   # this allows us to import from sibling directory\n",
    "\n",
    "from code_to_import.dataset_preprocessing import ProductionModeDataset\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "#import torchvision.transforms as transforms\n",
    "#from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # NOTE: I don't think this is used\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model in this run is threeLayerModel_ee_corrCut_0.8\n",
      "loaded correlations... shape is (21,)\n",
      "['lb_delta_eta', 'lbbar_delta_eta', 'lnu_delta_eta', 'lnubar_delta_eta', 'lbarb_delta_eta', 'lbarbbar_delta_eta', 'lbarnu_delta_eta', 'lbarnubar_delta_eta', 'bnu_delta_eta', 'bnubar_delta_eta', 'bbarnu_delta_eta', 'bbarnubar_delta_eta', 'lb_delta_phi', 'lbbar_delta_phi', 'lnu_delta_phi', 'lnubar_delta_phi', 'lbarb_delta_phi', 'lbarbbar_delta_phi', 'lbarnu_delta_phi', 'lbarnubar_delta_phi', 'bnu_delta_phi', 'bnubar_delta_phi', 'bbarnu_delta_phi', 'bbarnubar_delta_phi', 'wplusb_delta_eta', 'wplusbbar_delta_eta', 'wminusb_delta_eta', 'wminusbbar_delta_eta', 'wplusb_delta_phi', 'wplusbbar_delta_phi', 'wminusb_delta_phi', 'wminusbbar_delta_phi', 'top_eta', 'top_boosted_eta', 'tbar_eta', 'tbar_boosted_eta', 'ttbar_delta_eta', 'ttbar_eta', 'llbar_delta_eta', 'bbbar_delta_eta', 'nunubar_delta_eta', 'top_phi', 'tbar_phi', 'ttbar_phi', 'ttbar_delta_phi', 'llbar_phi', 'llbar_delta_phi', 'bbbar_phi', 'bbbar_delta_phi', 'nunubar_phi', 'nunubar_delta_phi', 'l_eta', 'lbar_eta', 'l_phi', 'lbar_phi', 'b_eta', 'bbar_eta', 'b_phi', 'bbar_phi', 'nu_eta', 'nubar_eta', 'nu_phi', 'nubar_phi', 'wplus_eta', 'wminus_eta', 'wplus_phi', 'wminus_phi', 'top_pt', 'tbar_pt', 'l_pt', 'b_pt', 'bbar_pt', 'nu_pt', 'nubar_pt', 'met_pt', 'ttbar_pt', 'ttbar_boosted_pt', 'wplus_pt', 'wminus_pt', 'ttbar_mass', 'production_mode', 'eventWeight', '__index__']\n",
      "num qqbar = 6324\n",
      "training (15367, 62)\n",
      "evaluating (3605, 62)\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "######################## THIS CLASS INCLUDES ALL THE VARIABLES YOU WANT TO CONFIGURE #################################\n",
    "#######################################################################################################################\n",
    "\n",
    "class opt():   # Class used for optimizers in the future. Defines all variables and stuff needed.\n",
    "#     save_weights = True  # Tells whether to save weights... currently not used\n",
    "    \n",
    "    load_model = False  # set true if you want to load and continue training a previous model\n",
    "    \n",
    "    draw = True # set to false when running on slurm\n",
    "    \n",
    "    n_epochs = 40000   # an epoch is the number of times it works through the entire training set.\n",
    "                       # This sets the total number of epochs which will be run\n",
    "    \n",
    "    batch_size = 5000   # the training set is broken up into batches, \n",
    "                        # this sets the size of each batch\n",
    "    \n",
    "    lr = 0.001   # learning rate (how much to change based on error)\n",
    "    b1 = 0.9   # Used for Adam. Exponential decay rate for the first moment\n",
    "    b2 = 0.999   # Used for Adam. Exponential decay rate for the second moment estimates (gradient squared)\n",
    "        \n",
    "    correlation_cut = -1  # this is the correlation cut... If negative then no cut is applied\n",
    "    \n",
    "    # the root_path leads to the folder with the root files being used for data\n",
    "    root_path = \"/depot-new/cms/top/mcnama20/TopSpinCorr-Run2-Entanglement/CMSSW_10_2_22/src/TopAnalysis/Configuration/analysis/diLeptonic/three_files/Nominal\"\n",
    "\n",
    "    file = root_path + \"/ee_modified_root_1.root\"   # this is the data root file loaded into the dataloader\n",
    "    \n",
    "    model_name = \"threeLayerModel_ee_corrCut_\" + str(correlation_cut) # this is the model name. \n",
    "                                                                          # Change it when running a new model\n",
    "\n",
    "print(\"The model in this run is \" + opt.model_name)   # this will make slurm output easier to identify\n",
    "\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "############################## Configure data loader depending on if there is a correlation cut ###########################\n",
    "if opt.correlation_cut > 0:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        ProductionModeDataset(opt.file, correlation_cut=opt.correlation_cut),\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "else:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        ProductionModeDataset(opt.file),\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "print('done')\n",
    "\n",
    "data = iter(dataloader)\n",
    "x = data.next()\n",
    "input_size = x.shape[1] -3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    classifier layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()   # Just uses the module constructor with name Discriminator \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),   # first layer\n",
    "            nn.LeakyReLU(0.2, inplace=True),   # apply leaky relu to layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 3),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        applies model to input and attempts to classify\n",
    "        \"\"\"\n",
    "        output = self.model(input)   # Classifies the input (at location) as gg (0) qqbar (1) or other (2)\n",
    "        return output\n",
    "\n",
    "\n",
    "# ******* OUT OF CLASSES NOW ************\n",
    "\n",
    "############### Initialize classifier and load a model if needed ##########################\n",
    "classifier = Classifier()\n",
    "if opt.load_model:\n",
    "    classifier.load_state_dict(torch.load(\"../models/three_layers/\" + opt.model_name + \".pt\")) #load module with same name\n",
    "    classifier.train()  # set the model up for training\n",
    "if cuda:\n",
    "    classifier.cuda()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded correlations... shape is (21,)\n",
      "['lb_delta_eta', 'lbbar_delta_eta', 'lnu_delta_eta', 'lnubar_delta_eta', 'lbarb_delta_eta', 'lbarbbar_delta_eta', 'lbarnu_delta_eta', 'lbarnubar_delta_eta', 'bnu_delta_eta', 'bnubar_delta_eta', 'bbarnu_delta_eta', 'bbarnubar_delta_eta', 'lb_delta_phi', 'lbbar_delta_phi', 'lnu_delta_phi', 'lnubar_delta_phi', 'lbarb_delta_phi', 'lbarbbar_delta_phi', 'lbarnu_delta_phi', 'lbarnubar_delta_phi', 'bnu_delta_phi', 'bnubar_delta_phi', 'bbarnu_delta_phi', 'bbarnubar_delta_phi', 'wplusb_delta_eta', 'wplusbbar_delta_eta', 'wminusb_delta_eta', 'wminusbbar_delta_eta', 'wplusb_delta_phi', 'wplusbbar_delta_phi', 'wminusb_delta_phi', 'wminusbbar_delta_phi', 'top_eta', 'top_boosted_eta', 'tbar_eta', 'tbar_boosted_eta', 'ttbar_delta_eta', 'ttbar_eta', 'llbar_delta_eta', 'bbbar_delta_eta', 'nunubar_delta_eta', 'top_phi', 'tbar_phi', 'ttbar_phi', 'ttbar_delta_phi', 'llbar_phi', 'llbar_delta_phi', 'bbbar_phi', 'bbbar_delta_phi', 'nunubar_phi', 'nunubar_delta_phi', 'l_eta', 'lbar_eta', 'l_phi', 'lbar_phi', 'b_eta', 'bbar_eta', 'b_phi', 'bbar_phi', 'nu_eta', 'nubar_eta', 'nu_phi', 'nubar_phi', 'wplus_eta', 'wminus_eta', 'wplus_phi', 'wminus_phi', 'top_pt', 'tbar_pt', 'l_pt', 'b_pt', 'bbar_pt', 'nu_pt', 'nubar_pt', 'met_pt', 'ttbar_pt', 'ttbar_boosted_pt', 'wplus_pt', 'wminus_pt', 'ttbar_mass', 'production_mode', 'eventWeight', '__index__']\n",
      "num qqbar = 6324\n",
      "training (15367, 62)\n",
      "evaluating (3605, 62)\n"
     ]
    }
   ],
   "source": [
    "####################################### load data for evaluation of model (not training set) and separate weights and target\n",
    "if opt.correlation_cut > 0:\n",
    "    validation_data = ProductionModeDataset(opt.file, correlation_cut=opt.correlation_cut).get_eval_data()\n",
    "else:\n",
    "    validation_data = ProductionModeDataset(opt.file).get_eval_data()\n",
    "\n",
    "w_val = validation_data[:,input_size + 1]\n",
    "w_val = Variable(torch.from_numpy(w_val).type(torch.FloatTensor))\n",
    "target_val = validation_data[:,input_size]\n",
    "target_val = Variable(torch.from_numpy(target_val).type(torch.LongTensor))\n",
    "y_val = np.transpose(validation_data)\n",
    "y_val = np.delete(y_val, [input_size, input_size + 1, input_size + 2], 0)\n",
    "y_val = np.transpose(y_val)\n",
    "val_data = Variable(torch.from_numpy(y_val).type(torch.Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAZH0lEQVR4nO3df3BV5b3v8ffXEIkoVIToQUIhvQPFkECAENNLBUstRuAK/qjSXgWqglLHW0+PKBw6gG3P1KOMcpkKHagKtFyBq4JcxaqgqdLyYxJE+X0ARc2BHhALjSI5At/7x17khLiT7PzcSZ/Pa2ZN1n7Ws9Z+vuyZDyvPWnvF3B0REQnDeckegIiINB+FvohIQBT6IiIBUeiLiAREoS8iEpA2yR5AbTp37uw9evRI9jBERFqVkpKST9w9vWp7iw/9Hj16UFxcnOxhiIi0Kmb2Ybx2Te+IiAREoS8iEhCFvohIQFr8nL6INL0vv/yS0tJSTp48meyhSB2lpaWRkZFBampqQv0V+iJCaWkp7du3p0ePHphZsocjCXJ3jh49SmlpKZmZmQnto+kdEeHkyZN06tRJgd/KmBmdOnWq029oCn0RAVDgt1J1/dwU+iIiAVHoi0jSHTt2jHnz5tVr3xEjRnDs2LEa+8yYMYO1a9fW6/hV9ejRg08++aRRjpUMCn0RSbqaQv/06dM17rtmzRouvvjiGvv8/Oc/55prrqnv8P6uKPRFJOmmTp3K/v37yc3NZcqUKRQVFfGd73yHH/7wh+Tk5AAwZswYBg4cSJ8+fViwYEHFvmfPvA8cOMAVV1zBxIkT6dOnD8OHD+eLL74AYMKECTz33HMV/WfOnMmAAQPIyclh9+7dABw5coTvfe97DBgwgLvvvpvu3bvXekb/+OOPk52dTXZ2NnPmzAHg888/Z+TIkfTr14/s7GyWL19eUWNWVhZ9+/blgQceaNR/v7rQLZsico6H/98Odh78W6MeM+vyDsz8H32q3f7II4+wfft2tm7dCkBRURGbN29m+/btFbciPv3001xyySV88cUXDBo0iJtuuolOnTqdc5y9e/fy7LPPsnDhQm655Raef/55brvttq+8X+fOndmyZQvz5s1j9uzZ/Pa3v+Xhhx9m2LBhTJs2jT/84Q/n/McST0lJCc888wybNm3C3bnyyisZOnQo77//Ppdffjkvv/wyAMePH+fTTz9l5cqV7N69GzOrdTqqKelMX0RapPz8/HPuPZ87dy79+vWjoKCAjz/+mL17935ln8zMTHJzcwEYOHAgBw4ciHvsG2+88St91q9fz9ixYwEoLCykY8eONY5v/fr13HDDDVx44YVcdNFF3Hjjjbz99tvk5OSwdu1aHnroId5++22+9rWv0aFDB9LS0rjrrrt44YUXaNeuXR3/NRqPzvRF5Bw1nZE3pwsvvLBivaioiLVr17JhwwbatWvH1VdfHffe9LZt21asp6SkVEzvVNcvJSWFU6dOAbEvOtVFdf179epFSUkJa9asYdq0aQwfPpwZM2awefNm1q1bx7Jly/j1r3/NG2+8Uaf3ayw60xeRpGvfvj1lZWXVbj9+/DgdO3akXbt27N69m40bNzb6GL797W+zYsUKAF577TX++te/1th/yJAhrFq1ihMnTvD555+zcuVKrrrqKg4ePEi7du247bbbeOCBB9iyZQufffYZx48fZ8SIEcyZM6diGisZdKYvIknXqVMnBg8eTHZ2Ntdddx0jR448Z3thYSG/+c1v6Nu3L9/85jcpKCho9DHMnDmTH/zgByxfvpyhQ4fSpUsX2rdvX23/AQMGMGHCBPLz8wG466676N+/P6+++ipTpkzhvPPOIzU1lfnz51NWVsbo0aM5efIk7s4TTzzR6ONPlNX1V5rmlpeX5/ojKiJNa9euXVxxxRXJHkZSlZeXk5KSQps2bdiwYQOTJ09O6hl5XcT7/MysxN3zqvbVmb6ICPDRRx9xyy23cObMGc4//3wWLlyY7CE1CYW+iAjQs2dP3nnnnWQPo8npQq6ISEAU+iIiAVHoi4gERKEvIhIQhb6I/F1pyKOPV61axc6dOyteN9YjmYuKihg1alSDj9MYFPoiIpGqof/3+Ehmhb6ItAi///3vyc/PJzc3l7vvvpvTp08zf/58HnzwwYo+ixYt4r777gOqf9TyWQcOHCA7O7vi9ezZs5k1axYACxcuZNCgQfTr14+bbrqJEydO8Oc//5nVq1czZcoUcnNz2b9//zmPZF63bh39+/cnJyeHO+64g/LycqD6RzVX59NPP2XMmDH07duXgoIC3nvvPQD++Mc/kpubS25uLv3796esrIxDhw4xZMgQcnNzyc7O5u23367/P3Ck1vv0zSwNeAtoG/V/zt1nVtr+APAYkO7un0Rt04A7gdPA/3L3V6P2gcAi4AJgDfATb+lfCRYJzStT4S/bGveY/5AD1z1S7eZdu3axfPly/vSnP5GamsqPf/xjli5dys0338y3vvUtHn30UQCWL1/O9OnTgcQetVydG2+8kYkTJwLws5/9jKeeeor77ruP66+/nlGjRnHzzTef0//kyZNMmDCBdevW0atXL8aNG8f8+fO5//77gfiPaq7OzJkz6d+/P6tWreKNN95g3LhxbN26ldmzZ/Pkk08yePBgPvvsM9LS0liwYAHXXnst06dP5/Tp05w4cSKh+mqSyJl+OTDM3fsBuUChmRUAmFk34HvAR2c7m1kWMBboAxQC88wsJdo8H5gE9IyWwgZXICKt3rp16ygpKWHQoEHk5uaybt063n//fdLT0/nGN77Bxo0bOXr0KHv27GHw4MFAYo9ars727du56qqryMnJYenSpezYsaPG/nv27CEzM5NevXoBMH78eN56662K7fEe1Vyd9evXc/vttwMwbNgwjh49yvHjxxk8eDA//elPmTt3LseOHaNNmzYMGjSIZ555hlmzZrFt27YanwWUqFrP9KMz8c+il6nRcvbs/AngQeDFSruMBpa5eznwgZntA/LN7ADQwd03AJjZEmAM8EqDqxCRxlPDGXlTcXfGjx/Pr371q69su/XWW1mxYgW9e/fmhhtuwMwSetRymzZtOHPmTMXrytsnTJjAqlWr6NevH4sWLaKoqKjW8dUk3qOa63IsM2Pq1KmMHDmSNWvWUFBQwNq1axkyZAhvvfUWL7/8MrfffjtTpkxh3LhxNR6/NgnN6ZtZipltBQ4Dr7v7JjO7Hvh3d3+3SveuwMeVXpdGbV2j9art8d5vkpkVm1nxkSNHEqtERFqt7373uzz33HMcPnwYiM17f/jhh0DsLHrVqlU8++yz3HrrrUBij1q+7LLLOHz4MEePHqW8vJyXXnqpYltZWRldunThyy+/ZOnSpRXt1T3iuXfv3hw4cIB9+/YB8Lvf/Y6hQ4fWq9YhQ4ZUvGdRURGdO3emQ4cO7N+/n5ycHB566CHy8vLYvXs3H374IZdeeikTJ07kzjvvZMuWLfV6z8oSevaOu58Gcs3sYmClmfUFpgPD43S3eIeooT3e+y0AFkDsKZuJjFFEWq+srCx++ctfMnz4cM6cOUNqaipPPvkk3bt3p2PHjmRlZbFz586Kxxgn8qjl1NRUZsyYwZVXXklmZia9e/eu2PaLX/yCK6+8ku7du5OTk1MR9GPHjmXixInMnTu34gIuQFpaGs888wzf//73OXXqFIMGDeKee+6pV62zZs3iRz/6EX379qVdu3YsXrwYgDlz5vDmm2+SkpJCVlYW1113HcuWLeOxxx4jNTWViy66iCVLltTrPSur86OVzWwmcAa4Dzh7VSEDOAjkAz8CcPdfRf1fBWYBB4A33b131P4D4Gp3v7um99OjlUWanh6t3LrV5dHKtU7vmFl6dIaPmV0AXAO84+6XunsPd+9BbKpmgLv/BVgNjDWztmaWSeyC7WZ3PwSUmVmBmRkwjnOvBYiISBNLZHqnC7A4ugPnPGCFu79UXWd332FmK4CdwCng3mh6CGAy/3XL5ivoIq6ISLNK5O6d94D+tfTpUeX1vwD/EqdfMZBdtV1Eks/dif0SLq1JXafo9Y1cESEtLY2jR4/WOUAkudydo0ePkpaWlvA++stZIkJGRgalpaXoFunWJy0tjYyMjIT7K/RFhNTUVDIzM5M9DGkGmt4REQmIQl9EJCAKfRGRgCj0RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAXEQmIQl9EJCAKfRGRgCj0RUQCotAXEQlIraFvZmlmttnM3jWzHWb2cNT+mJntNrP3zGylmV1caZ9pZrbPzPaY2bWV2gea2bZo21wzsyapSkRE4krkTL8cGObu/YBcoNDMCoDXgWx37wv8GzANwMyygLFAH6AQmGdmKdGx5gOTgJ7RUth4pYiISG1qDX2P+Sx6mRot7u6vufupqH0jkBGtjwaWuXu5u38A7APyzawL0MHdN7i7A0uAMY1Yi4iI1CKhOX0zSzGzrcBh4HV331Slyx3AK9F6V+DjSttKo7au0XrV9njvN8nMis2s+MiRI4kMUUREEpBQ6Lv7aXfPJXY2n29m2We3mdl04BSw9GxTvEPU0B7v/Ra4e56756WnpycyRBERSUCd7t5x92NAEdFcvJmNB0YB/zOasoHYGXy3SrtlAAej9ow47SIi0kwSuXsn/eydOWZ2AXANsNvMCoGHgOvd/USlXVYDY82srZllErtgu9ndDwFlZlYQ3bUzDnixccsREZGatEmgTxdgcXQHznnACnd/ycz2AW2B16M7Lze6+z3uvsPMVgA7iU373Ovup6NjTQYWARcQuwbwCiIi0mzsv2ZlWqa8vDwvLi5O9jBERFoVMytx97yq7fpGrohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEpBaQ9/M0sxss5m9a2Y7zOzhqP0SM3vdzPZGPztW2meame0zsz1mdm2l9oFmti3aNtfMrGnKEhGReBI50y8Hhrl7PyAXKDSzAmAqsM7dewLroteYWRYwFugDFALzzCwlOtZ8YBLQM1oKG68UERGpTa2h7zGfRS9To8WB0cDiqH0xMCZaHw0sc/dyd/8A2Afkm1kXoIO7b3B3B5ZU2kdERJpBQnP6ZpZiZluBw8Dr7r4JuMzdDwFEPy+NuncFPq60e2nU1jVar9oe7/0mmVmxmRUfOXKkDuWIiEhNEgp9dz/t7rlABrGz9uwausebp/ca2uO93wJ3z3P3vPT09ESGKCIiCajT3TvufgwoIjYX/x/RlA3Rz8NRt1KgW6XdMoCDUXtGnHYREWkmidy9k25mF0frFwDXALuB1cD4qNt44MVofTUw1szamlkmsQu2m6MpoDIzK4ju2hlXaR8REWkGbRLo0wVYHN2Bcx6wwt1fMrMNwAozuxP4CPg+gLvvMLMVwE7gFHCvu5+OjjUZWARcALwSLSIi0kwsdiNNy5WXl+fFxcXJHoaISKtiZiXunle1Xd/IFREJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAlJr6JtZNzN708x2mdkOM/tJ1J5rZhvNbKuZFZtZfqV9ppnZPjPbY2bXVmofaGbbom1zzcyapiwREYknkTP9U8A/ufsVQAFwr5llAY8CD7t7LjAjek20bSzQBygE5plZSnSs+cAkoGe0FDZeKSIiUptaQ9/dD7n7lmi9DNgFdAUc6BB1+xpwMFofDSxz93J3/wDYB+SbWRegg7tvcHcHlgBjGrMYERGpWZu6dDazHkB/YBNwP/Cqmc0m9p/Hf4+6dQU2VtqtNGr7Mlqv2h7vfSYR+42Ar3/963UZooiI1CDhC7lmdhHwPHC/u/8NmAz8o7t3A/4ReOps1zi7ew3tX210X+Duee6el56enugQRUSkFgmFvpmlEgv8pe7+QtQ8Hji7/n+BsxdyS4FulXbPIDb1UxqtV20XEZFmksjdO0bsLH6Xuz9eadNBYGi0PgzYG62vBsaaWVszyyR2wXazux8CysysIDrmOODFRqpDREQSkMic/mDgdmCbmW2N2v4ZmAj8bzNrA5wkmoN39x1mtgLYSezOn3vd/XS032RgEXAB8Eq0iIhIM7HYjTQtV15enhcXFyd7GCIirYqZlbh7XtV2fSNXRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJiEJfRCQgCn0RkYAo9EVEAqLQFxEJSK2hb2bdzOxNM9tlZjvM7CeVtt1nZnui9kcrtU8zs33RtmsrtQ80s23RtrlmZo1fkoiIVKdNAn1OAf/k7lvMrD1QYmavA5cBo4G+7l5uZpcCmFkWMBboA1wOrDWzXu5+GpgPTAI2AmuAQuCVxi5KRETiq/VM390PufuWaL0M2AV0BSYDj7h7ebTtcLTLaGCZu5e7+wfAPiDfzLoAHdx9g7s7sAQY09gFiYhI9eo0p29mPYD+wCagF3CVmW0ysz+a2aCoW1fg40q7lUZtXaP1qu3x3meSmRWbWfGRI0fqMkQREalBwqFvZhcBzwP3u/vfiE0NdQQKgCnAimiOPt48vdfQ/tVG9wXunufueenp6YkOUUREapFQ6JtZKrHAX+ruL0TNpcALHrMZOAN0jtq7Vdo9AzgYtWfEaRcRkWaSyN07BjwF7HL3xyttWgUMi/r0As4HPgFWA2PNrK2ZZQI9gc3ufggoM7OC6JjjgBcbsxgREalZInfvDAZuB7aZ2dao7Z+Bp4GnzWw78J/A+OgC7Q4zWwHsJHbnz73RnTsQu/i7CLiA2F07unNHRKQZWSynW668vDwvLi5O9jBERFoVMytx97yq7fpGrohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQFr8X84ysyPAh8keRx11Jvb3gkOimsOgmluP7u6eXrWxxYd+a2RmxfH+TNnfM9UcBtXc+ml6R0QkIAp9EZGAKPSbxoJkDyAJVHMYVHMrpzl9EZGA6ExfRCQgCn0RkYAo9OvJzC4xs9fNbG/0s2M1/QrNbI+Z7TOzqXG2P2Bmbmadm37UDdPQms3sMTPbbWbvmdlKM7u42QZfRwl8bmZmc6Pt75nZgET3bYnqW6+ZdTOzN81sl5ntMLOfNP/o66chn3G0PcXM3jGzl5pv1I3A3bXUYwEeBaZG61OBf43TJwXYD3wDOB94F8iqtL0b8CqxL591TnZNTV0zMBxoE63/a7z9W8JS2+cW9RkBvAIYUABsSnTflrY0sN4uwIBovT3wby293obWXGn7T4H/A7yU7HrqsuhMv/5GA4uj9cXAmDh98oF97v6+u/8nsCza76wngAeB1nI1vUE1u/tr7n4q6rcRyGja4dZbbZ8b0eslHrMRuNjMuiS4b0tT73rd/ZC7bwFw9zJgF9C1OQdfTw35jDGzDGAk8NvmHHRjUOjX32Xufggg+nlpnD5dgY8rvS6N2jCz64F/d/d3m3qgjahBNVdxB7GzqJYokRqq65No/S1JQ+qtYGY9gP7ApsYfYqNraM1ziJ2wnWmi8TWZNskeQEtmZmuBf4izaXqih4jT5mbWLjrG8PqOrak0Vc1V3mM6cApYWrfRNZtaa6ihTyL7tjQNqTe20ewi4Hngfnf/WyOOranUu2YzGwUcdvcSM7u6sQfW1BT6NXD3a6rbZmb/cfbX2+hXvsNxupUSm7c/KwM4CPw3IBN418zOtm8xs3x3/0ujFVAPTVjz2WOMB0YB3/VoYrQFqrGGWvqcn8C+LU1D6sXMUokF/lJ3f6EJx9mYGlLzzcD1ZjYCSAM6mNnv3f22Jhxv40n2RYXWugCPce5FzUfj9GkDvE8s4M9eLOoTp98BWseF3AbVDBQCO4H0ZNdSS521fm7E5nMrX+TbXJfPvCUtDazXgCXAnGTX0Vw1V+lzNa3sQm7SB9BaF6ATsA7YG/28JGq/HFhTqd8IYnc07AemV3Os1hL6DaoZ2EdsjnRrtPwm2TXVUOtXagDuAe6J1g14Mtq+Dciry2fe0pb61gt8m9i0yHuVPtcRya6nqT/jSsdodaGvxzCIiAREd++IiAREoS8iEhCFvohIQBT6IiIBUeiLiAREoS8iEhCFvohIQP4/IakCeJV+EcIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3379.1016\n",
      "3298.6057\n",
      "3298.874\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-e243e9c25aa9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m###############################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Loop through all epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# x is a batch and there are i batches in the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#-----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################### Initialize (and load if needed) optimizer and loss function ################################\n",
    "################################################################################################################\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) # set up optimizer\n",
    "if opt.load_model:\n",
    "    optimizer.load_state_dict(torch.load(\"../models/three_layers/optimizer_\" + opt.model_name + \".pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)# this is the loss function. reduce=false makes it return a value for each input\n",
    "\n",
    "##################################################################################################\n",
    "################ initialize stuff before training ################################################\n",
    "\n",
    "small_loss = 1e20   # This is the initail loss under which we overwrite the model.\n",
    "                    # initialize with a large loss so everything is smaller than it\n",
    "\n",
    "# initialize loss arrays\n",
    "loss_val_array = np.array(())\n",
    "loss_array = np.array(())\n",
    "lva = 0  # this is the length of the loaded array\n",
    "la = 0  # this is the length of the other loaded array\n",
    "\n",
    "# load arrays and reset small_loss if loading model:\n",
    "if opt.load_model:\n",
    "    loss_val_array = np.load(\"../data/three_layers/\" + opt.model_name +  \"_loss_val_array.npy\")\n",
    "    lva = len(loss_val_array)\n",
    "\n",
    "    loss_array = np.load(\"../data/three_layers/\" + opt.model_name +  \"_loss_train_array.npy\")\n",
    "    la = len(loss_array)\n",
    "    \n",
    "    small_loss = np.min(loss_val_array)\n",
    "    \n",
    "\n",
    "batches_done = 0   # Counter for batches\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# ******************************** START TRAINING LOOP *******************************************************\n",
    "###############################################################################################################\n",
    "for epoch in range(opt.n_epochs):   # Loop through all epochs\n",
    "    for i, x in enumerate(dataloader): # x is a batch and there are i batches in the epoch\n",
    "        \n",
    "        #-----------------------------\n",
    "        # Configure input\n",
    "        #----------------------------\n",
    "        variable_len = len(x[0])\n",
    "        weight = x[:, variable_len-2]\n",
    "        weight = Variable(weight.type(torch.FloatTensor))\n",
    "        target = x[:, variable_len-3]\n",
    "        target = Variable(target.type(torch.LongTensor))\n",
    "        x = np.transpose(x)\n",
    "        x = np.delete(x, [variable_len-3, variable_len-2, variable_len-1], 0)\n",
    "        x = np.transpose(x)\n",
    "        batch = Variable(x.type(torch.Tensor))   # Variable is a wrapper for the Tensor x was just made into\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Classifier\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer.zero_grad()   # Make gradients zero so they don't accumulate\n",
    "        \n",
    "        output = classifier(batch)   # apply nn to input   \n",
    "\n",
    "        # Calculate loss \n",
    "        loss_l = criterion(output, target) # loss_l is a vector with the loss for each event in the batch\n",
    "        loss = torch.dot(weight,loss_l)   # we take the dot product with the weights to calculate the final loss\n",
    "        loss.backward()   # Do back propagation \n",
    "        optimizer.step()   # Update parameters based on gradients for individuals\n",
    "        batches_done += 1  # increase the number of batches in the counter\n",
    "        \n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # Save and Draw Stuff  (if drawing is on)\n",
    "    #-----------------------------------------\n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.detach().numpy())\n",
    "        loss_array = np.append(loss_array, loss.detach().numpy()) # append the training loss to the loss array\n",
    "        \n",
    "        \n",
    "        out = classifier(val_data)   # run classifier on validation data to see how good it is\n",
    "        loss_val = torch.dot(w_val, criterion(out, target_val))   # calculate the validation loss\n",
    "        loss_val_array = np.append(loss_val_array, loss_val.detach().numpy()) # append the validation loss to its array\n",
    "        \n",
    "        if small_loss > loss_val:   # compare to see if the loss has decreased\n",
    "            small_loss = loss_val   # if the network has improved replace the best loss with this one\n",
    "            torch.save(classifier.state_dict(), \"../models/three_layers/\" + opt.model_name + \".pt\")   # save the new (and better) model\n",
    "            torch.save(optimizer.state_dict(), \"../models/three_layers/optimizer_\" + opt.model_name + \".pt\") # save the optimizer state\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            # save the loss arrays\n",
    "            np.save(\"../data/three_layers/\" + opt.model_name +  \"_loss_val_array.npy\",loss_val_array)\n",
    "            np.save(\"../data/three_layers/\" + opt.model_name + \"_loss_train_array.npy\",loss_array)\n",
    "            if opt.draw:\n",
    "                #----------------------------------\n",
    "                # Draw training and validation loss\n",
    "                #-------------------------------------\n",
    "                display.clear_output(True)\n",
    "                figure = plt.figure()\n",
    "                ax = figure.add_subplot(111)\n",
    "                #ax.set_yscale(\"log\")\n",
    "                ax.plot(np.array(list(range(int((epoch)/10)+lva+1))), loss_array, label=\"training loss\")\n",
    "                ax.plot(np.array(list(range(int((epoch)/10)+la+1))), loss_val_array, label = \"evaluation loss\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: GANS_7]",
   "language": "python",
   "name": "conda-env-GANS_7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
