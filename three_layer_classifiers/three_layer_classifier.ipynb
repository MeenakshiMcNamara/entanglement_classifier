{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "######################################### Import statements ###########################################\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")   # this allows us to import from sibling directory\n",
    "\n",
    "from code_to_import.dataset_preprocessing import ProductionModeDataset\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "#import torchvision.transforms as transforms\n",
    "#from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # NOTE: I don't think this is used\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded correlations... shape is (6,)\n",
      "['lb_delta_eta', 'lbbar_delta_eta', 'lnu_delta_eta', 'lnubar_delta_eta', 'lbarb_delta_eta', 'lbarbbar_delta_eta', 'lbarnu_delta_eta', 'lbarnubar_delta_eta', 'bnu_delta_eta', 'bnubar_delta_eta', 'bbarnu_delta_eta', 'bbarnubar_delta_eta', 'lb_delta_phi', 'lbbar_delta_phi', 'lnu_delta_phi', 'lnubar_delta_phi', 'lbarb_delta_phi', 'lbarbbar_delta_phi', 'lbarnu_delta_phi', 'lbarnubar_delta_phi', 'bnu_delta_phi', 'bnubar_delta_phi', 'bbarnu_delta_phi', 'bbarnubar_delta_phi', 'wplusb_delta_eta', 'wplusbbar_delta_eta', 'wminusb_delta_eta', 'wminusbbar_delta_eta', 'wplusb_delta_phi', 'wplusbbar_delta_phi', 'wminusb_delta_phi', 'wminusbbar_delta_phi', 'top_eta', 'top_boosted_eta', 'tbar_eta', 'tbar_boosted_eta', 'ttbar_delta_eta', 'ttbar_eta', 'llbar_delta_eta', 'bbbar_delta_eta', 'nunubar_delta_eta', 'top_phi', 'tbar_phi', 'ttbar_phi', 'ttbar_delta_phi', 'llbar_phi', 'llbar_delta_phi', 'bbbar_phi', 'bbbar_delta_phi', 'nunubar_phi', 'nunubar_delta_phi', 'l_eta', 'lbar_eta', 'l_phi', 'lbar_phi', 'b_eta', 'bbar_eta', 'b_phi', 'bbar_phi', 'nu_eta', 'nubar_eta', 'nu_phi', 'nubar_phi', 'wplus_eta', 'wminus_eta', 'wplus_phi', 'wminus_phi', 'top_pt', 'tbar_pt', 'l_pt', 'b_pt', 'bbar_pt', 'nu_pt', 'nubar_pt', 'met_pt', 'ttbar_pt', 'ttbar_boosted_pt', 'wplus_pt', 'wminus_pt', 'ttbar_mass', 'production_mode', 'eventWeight', '__index__']\n",
      "num qqbar = 6324\n",
      "training (15367, 77)\n",
      "evaluating (3605, 77)\n",
      "The model in this run is testthreeLayerModel_ee_corrCut_0.8_weights_no-neg\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "######################## THIS CLASS INCLUDES ALL THE VARIABLES YOU WANT TO CONFIGURE #################################\n",
    "#######################################################################################################################\n",
    "\n",
    "class opt():   # Class used for optimizers in the future. Defines all variables and stuff needed.\n",
    "#     save_weights = True  # Tells whether to save weights... currently not used\n",
    "    \n",
    "    load_model = False  # set true if you want to load and continue training a previous model\n",
    "    \n",
    "    draw = True # set to false when running on slurm\n",
    "    \n",
    "    n_epochs = 40000   # an epoch is the number of times it works through the entire training set.\n",
    "                       # This sets the total number of epochs which will be run\n",
    "    \n",
    "    batch_size = 5000   # the training set is broken up into batches, \n",
    "                        # this sets the size of each batch\n",
    "    \n",
    "    weight_cmd = \"no-neg\"   # could also be \"false\" and \"no-neg\".\n",
    "                          # This determines whether weights and negative weights are used\n",
    "    \n",
    "    lr = 0.0001   # learning rate (how much to change based on error)\n",
    "    b1 = 0.9   # Used for Adam. Exponential decay rate for the first moment\n",
    "    b2 = 0.999   # Used for Adam. Exponential decay rate for the second moment estimates (gradient squared)\n",
    "        \n",
    "    correlation_cut = 0.8  # this is the correlation cut... If negative then no cut is applied\n",
    "    \n",
    "    # the root_path leads to the folder with the root files being used for data\n",
    "    root_path = \"/depot/cms/top/mcnama20/TopSpinCorr-Run2-Entanglement/CMSSW_10_2_22/src/TopAnalysis/Configuration/analysis/diLeptonic/three_files/Nominal\"\n",
    "\n",
    "    file = root_path + \"/ee_modified_root_1.root\"   # this is the data root file loaded into the dataloader\n",
    "    \n",
    "    # this is the model name. Change it when running a new model\n",
    "    model_name = \"testthreeLayerModel_ee_corrCut_\" + str(correlation_cut)  + \"_weights_\" + weight_cmd\n",
    "    \n",
    "    # load data object so we can access validation and training data\n",
    "    if correlation_cut > 0:\n",
    "        data = ProductionModeDataset(file, correlation_cut=correlation_cut)\n",
    "    else:\n",
    "        data = ProductionModeDataset(file)\n",
    "\n",
    "print(\"The model in this run is \" + opt.model_name)   # this will make slurm output easier to identify\n",
    "\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "############################## Configure data loader depending on if there is a correlation cut ###########################\n",
    "if opt.correlation_cut > 0:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        opt.data,\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "else:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        opt.data,\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "print('done')\n",
    "\n",
    "data = iter(dataloader)\n",
    "x = data.next()\n",
    "input_size = x.shape[1] -3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader._SingleProcessDataLoaderIter at 0x7f767c0af7f0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = iter(dataloader)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Now save training and validation sets for future analysis ###################################\n",
    "np.save(\"../data/three_layers/train_val_datasets/training_dataset_\" + opt.model_name + \".npy\", opt.data.get_training_data())\n",
    "np.save(\"../data/three_layers/train_val_datasets/validation_dataset_\" + opt.model_name + \".npy\", opt.data.get_eval_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    classifier layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()   # Just uses the module constructor with name Discriminator \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),   # first layer\n",
    "            nn.BatchNorm1d(512),   # batch normalization\n",
    "            nn.LeakyReLU(0.2, inplace=True),   # apply leaky relu to layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),# batch normalization\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 3),\n",
    "            nn.BatchNorm1d(3),  # batch normalization\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        applies model to input and attempts to classify\n",
    "        \"\"\"\n",
    "        output = self.model(input)   # Classifies the input (at location) as gg (0) qqbar (1) or other (2)\n",
    "        return output\n",
    "\n",
    "\n",
    "# ******* OUT OF CLASSES NOW ************\n",
    "\n",
    "############### Initialize classifier and load a model if needed ##########################\n",
    "classifier = Classifier()\n",
    "if opt.load_model:\n",
    "    classifier.load_state_dict(torch.load(\"../models/three_layers/\" + opt.model_name + \".pt\")) #load module with same name\n",
    "    classifier.train()  # set the model up for training\n",
    "if cuda:\n",
    "    classifier.cuda()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################### load data for evaluation of model (not training set) and separate weights and target\n",
    "validation_data = opt.data.get_eval_data()\n",
    "\n",
    "w_val = validation_data[:,input_size + 1]\n",
    "\n",
    "target_val = validation_data[:,input_size]\n",
    "target_val = Variable(torch.from_numpy(target_val).type(torch.LongTensor))\n",
    "y_val = np.transpose(validation_data)\n",
    "y_val = np.delete(y_val, [input_size, input_size + 1, input_size + 2], 0)\n",
    "y_val = np.transpose(y_val)\n",
    "val_data = Variable(torch.from_numpy(y_val).type(torch.Tensor))\n",
    "\n",
    " # replace all negative weighted events with zero if the weight_cmd says to\n",
    "if opt.weight_cmd == \"no-neg\":\n",
    "    w_val[w_val < 0] = 0\n",
    "\n",
    "# remove weighting (aka, set all to 1) if weight_cmd says to\n",
    "if opt.weight_cmd == \"false\":\n",
    "    w_val = np.ones(w_val.shape)\n",
    "    \n",
    "w_val = Variable(torch.from_numpy(w_val).type(torch.FloatTensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3377.3499\n",
      "3265.1855\n"
     ]
    }
   ],
   "source": [
    "################### Initialize (and load if needed) optimizer and loss function ################################\n",
    "################################################################################################################\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) # set up optimizer\n",
    "if opt.load_model:\n",
    "    optimizer.load_state_dict(torch.load(\"../models/three_layers/optimizer_\" + opt.model_name + \".pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)# this is the loss function. reduce=false makes it return a value for each input\n",
    "\n",
    "##################################################################################################\n",
    "################ initialize stuff before training ################################################\n",
    "\n",
    "small_loss = 1e20   # This is the initail loss under which we overwrite the model.\n",
    "                    # initialize with a large loss so everything is smaller than it\n",
    "\n",
    "# initialize loss arrays\n",
    "loss_val_array = np.array(())\n",
    "loss_array = np.array(())\n",
    "lva = 0  # this is the length of the loaded array\n",
    "la = 0  # this is the length of the other loaded array\n",
    "\n",
    "# load arrays and reset small_loss if loading model:\n",
    "if opt.load_model:\n",
    "    loss_val_array = np.load(\"../data/three_layers/\" + opt.model_name +  \"_loss_val_array.npy\")\n",
    "    lva = len(loss_val_array)\n",
    "\n",
    "    loss_array = np.load(\"../data/three_layers/\" + opt.model_name +  \"_loss_train_array.npy\")\n",
    "    la = len(loss_array)\n",
    "    \n",
    "    small_loss = np.min(loss_val_array)\n",
    "    \n",
    "\n",
    "batches_done = 0   # Counter for batches\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# ******************************** START TRAINING LOOP *******************************************************\n",
    "###############################################################################################################\n",
    "for epoch in range(opt.n_epochs):   # Loop through all epochs\n",
    "    for i, x in enumerate(dataloader): # x is a batch and there are i batches in the epoch\n",
    "        \n",
    "        #-----------------------------\n",
    "        # Configure input\n",
    "        #----------------------------\n",
    "        variable_len = len(x[0])\n",
    "        weight = x[:, variable_len-2]\n",
    "        weight = Variable(weight.type(torch.FloatTensor))\n",
    "        target = x[:, variable_len-3]\n",
    "        target = Variable(target.type(torch.LongTensor))\n",
    "        x = np.transpose(x)\n",
    "        x = np.delete(x, [variable_len-3, variable_len-2, variable_len-1], 0)\n",
    "        x = np.transpose(x)\n",
    "        batch = Variable(x.type(torch.Tensor))   # Variable is a wrapper for the Tensor x was just made into\n",
    "        \n",
    "        # replace all negative weighted events with zero if the weight_cmd says to\n",
    "        if opt.weight_cmd == \"no-neg\":\n",
    "            weight[weight < 0] = 0\n",
    "            #weight = Variable(torch.from_numpy(weight).type(torch.FloatTensor))\n",
    "        \n",
    "        # remove weighting (aka, set all to 1) if weight_cmd says to\n",
    "        if opt.weight_cmd == \"false\":\n",
    "            weight = np.ones(weight.shape)\n",
    "            weight = Variable(torch.from_numpy(weight).type(torch.FloatTensor))\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Classifier\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer.zero_grad()   # Make gradients zero so they don't accumulate\n",
    "        \n",
    "        output = classifier(batch)   # apply nn to input   \n",
    "\n",
    "        # Calculate loss \n",
    "        loss_l = criterion(output, target) # loss_l is a vector with the loss for each event in the batch\n",
    "        loss = torch.dot(weight,loss_l)   # we take the dot product with the weights to calculate the final loss\n",
    "        loss.backward()   # Do back propagation \n",
    "        optimizer.step()   # Update parameters based on gradients for individuals\n",
    "        batches_done += 1  # increase the number of batches in the counter\n",
    "        \n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # Save and Draw Stuff  (if drawing is on)\n",
    "    #-----------------------------------------\n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.detach().numpy())\n",
    "        loss_array = np.append(loss_array, loss.detach().numpy()) # append the training loss to the loss array\n",
    "        \n",
    "        \n",
    "        out = classifier(val_data)   # run classifier on validation data to see how good it is\n",
    "        loss_val = torch.dot(w_val, criterion(out, target_val))   # calculate the validation loss\n",
    "        loss_val_array = np.append(loss_val_array, loss_val.detach().numpy()) # append the validation loss to its array\n",
    "        \n",
    "        if small_loss > loss_val:   # compare to see if the loss has decreased\n",
    "            small_loss = loss_val   # if the network has improved replace the best loss with this one\n",
    "            torch.save(classifier.state_dict(), \"../models/three_layers/\" + opt.model_name + \".pt\")   # save the new (and better) model\n",
    "            torch.save(optimizer.state_dict(), \"../models/three_layers/optimizer_\" + opt.model_name + \".pt\") # save the optimizer state\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            # save the loss arrays\n",
    "            np.save(\"../data/three_layers/\" + opt.model_name +  \"_loss_val_array.npy\",loss_val_array)\n",
    "            np.save(\"../data/three_layers/\" + opt.model_name + \"_loss_train_array.npy\",loss_array)\n",
    "            if opt.draw:\n",
    "                #----------------------------------\n",
    "                # Draw training and validation loss\n",
    "                #-------------------------------------\n",
    "                display.clear_output(True)\n",
    "                figure = plt.figure()\n",
    "                ax = figure.add_subplot(111)\n",
    "                #ax.set_yscale(\"log\")\n",
    "                ax.plot(10 * np.array(list(range(int((epoch)/10)+lva+1))), loss_array, label=\"training loss\")\n",
    "                ax.plot(10 * np.array(list(range(int((epoch)/10)+la+1))), loss_val_array, label = \"evaluation loss\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: GANS_7]",
   "language": "python",
   "name": "conda-env-GANS_7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
