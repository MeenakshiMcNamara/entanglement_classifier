{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This file will run any correlation cut and any channel type you want, and takes these in as command line arguments!\n",
    "\n",
    "Because this does everything we want, I am going to move the other versions into the old/ duplicate directory since they are no longer needed.\n",
    "\n",
    "If you want to add a version to the run then it will also take the version as a command line argument and it will be appended to the model name and everything saved which uses the model name. I will only leave one submission script in this directory, but you can find (and use by moving into this directory) other submission scripts so this can be run in parallel.\n",
    "\"\"\"\n",
    "\n",
    "# TODO: add more so continuing training works!!!!\n",
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[26]:\n",
    "\n",
    "######################################### Import statements ###########################################\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")   # this allows us to import from sibling directory\n",
    "\n",
    "from code_to_import.dataset_preprocessing import ProductionModeDataset\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "#import torchvision.transforms as transforms\n",
    "#from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # NOTE: I don't think this is used\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class args():\n",
    "    channel = \"ee\"\n",
    "    cut = -1\n",
    "    version = -1\n",
    "    cut_version = -1\n",
    "    weight = \"true\"\n",
    "    drop = 0.2\n",
    "    input = \"lorentzdelta_gen\"\n",
    "    includes_qg = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "threeLayerModel_ee_corrCut_-1_weights_true_drop_0.2_lorentzdelta_gen_no_qg\n",
      "['gen_l_eta', 'gen_lbar_eta', 'gen_l_phi', 'gen_lbar_phi', 'gen_l_pt', 'gen_lbar_pt', 'gen_l_mass', 'gen_lbar_mass', 'gen_b_eta', 'gen_bbar_eta', 'gen_b_phi', 'gen_bbar_phi', 'gen_b_pt', 'gen_bbar_pt', 'gen_b_mass', 'gen_bbar_mass', 'gen_llbar_delta_eta', 'gen_bbbar_delta_eta', 'gen_lbbar_delta_eta', 'gen_blbar_delta_eta', 'gen_lb_delta_eta', 'gen_bbarlbar_delta_eta', 'gen_llbar_delta_phi', 'gen_bbbar_delta_phi', 'gen_lbbar_delta_phi', 'gen_blbar_delta_phi', 'gen_lb_delta_phi', 'gen_bbarlbar_delta_phi', 'production_mode', 'eventWeight', '__index__']\n",
      "num qqbar = 52908\n",
      "training (85710, 31)\n",
      "evaluating (20105, 31)\n",
      "The model in this run is threeLayerModel_ee_corrCut_-1_weights_true_drop_0.2_lorentzdelta_gen_no_qg\n"
     ]
    }
   ],
   "source": [
    "######################## THIS CLASS INCLUDES ALL THE VARIABLES YOU WANT TO CONFIGURE #################################\n",
    "#######################################################################################################################\n",
    "\n",
    "class opt():   # Class used for optimizers in the future. Defines all variables and stuff needed.\n",
    "#     save_weights = True  # Tells whether to save weights... currently not used\n",
    "    \n",
    "    load_model = False  # set true if you want to load and continue training a previous model\n",
    "    \n",
    "    draw = True # set to false when running on slurm\n",
    "    \n",
    "    n_epochs = 40000   # an epoch is the number of times it works through the entire training set.\n",
    "                       # This sets the total number of epochs which will be run\n",
    "    \n",
    "    batch_size = 1   # the training set is broken up into batches, \n",
    "                        # this sets the size of each batch\n",
    "    \n",
    "    # this is the number of outputs for the neural network\n",
    "    output_num = 3\n",
    "    if args.includes_qg != \"true\":\n",
    "        output_num = 2\n",
    "    \n",
    "    lr = 0.0001   # learning rate (how much to change based on error)\n",
    "    b1 = 0.9   # Used for Adam. Exponential decay rate for the first moment\n",
    "    b2 = 0.999   # Used for Adam. Exponential decay rate for the second moment estimates (gradient squared)\n",
    "        \n",
    "    correlation_cut = args.cut   # this is the correlation cut... If negative then no cut is applied\n",
    "    \n",
    "    drop = args.drop   # percentage of nodes which will be dropped each time\n",
    "    \n",
    "    weight_cmd = args.weight   # could also be \"false\" and \"no-neg\".\n",
    "                          # This determines whether weights and negative weights are used\n",
    "    \n",
    "    qg_cmd = (args.includes_qg == \"true\")   # this is a boolean which determines if qg is included (true) or excluded (false)\n",
    "    \n",
    "    # the root_path leads to the folder with the root files being used for data\n",
    "    root_path = \"/depot/cms/top/mcnama20/TopSpinCorr-Run2-Entanglement/CMSSW_10_2_22/src/TopAnalysis/Configuration/analysis/diLeptonic/three_files/Nominal\"\n",
    "    \n",
    "    # this is the data root file loaded into the dataloader\n",
    "    file = root_path + \"/\" + args.channel\n",
    "    if args.input == \"all\":\n",
    "        file += \"_modified_root_1.root\"   \n",
    "    elif args.input == \"lorentz\":\n",
    "        file += \"_modified_root_1_lorentzvectors.root\"\n",
    "    elif args.input == \"spinCorr\":\n",
    "        file +=\"_modified_root_1_spinCorr.root\"\n",
    "    elif args.input == \"lorentzdelta\":\n",
    "        file += \"_modified_root_1_lorentzvectorsdelta.root\"\n",
    "    #elif args.input == \"lorentzgen\":\n",
    "     #   file += \"_modified_root_1_lorentzvectorsdelta_gen.root\"\n",
    "    elif args.input == \"lorentz_mttbar\":\n",
    "        file += \"_modified_root_1_lorentzvectors_mttbar_mass.root\"\n",
    "    elif args.input == \"lorentz_delta2\":\n",
    "        file += \"__modified_root_1_lorentzvectorsdelta_version2.root\"\n",
    "    elif args.input == \"lorentzdelta_gen\":\n",
    "        file += \"_modified_root_1_lorentzvectorsdelta_gen.root\"\n",
    "    \n",
    "    # this is the model name. Change it when running a new model\n",
    "    model_name = \"threeLayerModel_\" + args.channel + \"_corrCut_\" + str(correlation_cut)  + \"_weights_\" + weight_cmd + \"_drop_\" + str(drop)\n",
    "    \n",
    "    # add version information if included\n",
    "    if args.version > 0:\n",
    "        model_name += str(args.version)\n",
    "                    \n",
    "    # add cut version to model name if included in args\n",
    "    if args.cut_version > 0:\n",
    "        model_name += \"cutV\" + str(args.cut_version)\n",
    "        \n",
    "    # add input type. Can be \"all\", \"lorentz\" and \"spinCorr\"\n",
    "    if args.input != \"all\":\n",
    "        model_name += \"_\" + args.input\n",
    "        \n",
    "    if not qg_cmd:\n",
    "        model_name += \"_no_qg\"\n",
    "    \n",
    "    \n",
    "    print(model_name)\n",
    "    \n",
    "    # load data object so we can access validation and training data    \n",
    "    if correlation_cut > 0:\n",
    "        data = ProductionModeDataset(file, correlation_cut=correlation_cut, cut_version=args.cut_version, include_qg = qg_cmd)\n",
    "    else:\n",
    "        data = ProductionModeDataset(file, include_qg = qg_cmd)\n",
    "        \n",
    "\n",
    "print(\"The model in this run is \" + opt.model_name)   # this will make slurm output easier to identify\n",
    "\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = np.array([1,0,1])\n",
    "list = np.transpose(list)\n",
    "array = np.zeros((3,2))\n",
    "array[:,0] = 1-list\n",
    "array = array.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [1, 0],\n",
       "       [0, 0]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "############################## Configure data loader depending on if there is a correlation cut ###########################\n",
    "if opt.correlation_cut > 0:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        opt.data,\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "else:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        opt.data,\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True\n",
    "    )\n",
    "print('done')\n",
    "\n",
    "data = iter(dataloader)\n",
    "x = data.next()\n",
    "input_size = x.shape[1] -3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Now save training and validation sets for future analysis ###################################\n",
    "np.save(\"../data/three_layers/train_val_datasets/training_dataset_\" + opt.model_name + \".npy\", opt.data.get_training_data())\n",
    "np.save(\"../data/three_layers/train_val_datasets/validation_dataset_\" + opt.model_name + \".npy\", opt.data.get_eval_data())\n",
    "\n",
    "\n",
    "# In[29]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################## CLASSIFIER CLASS #############################################################\n",
    "#########################################################################################################################\n",
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    classifier layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()   # Just uses the module constructor with name Discriminator \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),   # first layer\n",
    "            nn.BatchNorm1d(512),   # batch normalization\n",
    "            nn.Dropout(opt.drop),   # add dropout\n",
    "            nn.LeakyReLU(0.2, inplace=True),   # apply leaky relu to layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.Dropout(opt.drop),   # add dropout\n",
    "            nn.BatchNorm1d(256),# batch normalization\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, opt.output_num),   # last layer\n",
    "            nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        applies model to input and attempts to classify\n",
    "        \"\"\"\n",
    "        output = self.model(input)   # Classifies the input (at location) as gg (0) qqbar (1) or other (2)\n",
    "        return output\n",
    "\n",
    "\n",
    "# ******* OUT OF CLASSES NOW ************\n",
    "\n",
    "############### Initialize classifier and load a model if needed ##########################\n",
    "classifier = Classifier()\n",
    "if opt.load_model:\n",
    "    classifier.load_state_dict(torch.load(\"../models/three_layers/\" + opt.model_name + \".pt\")) #load module with same name\n",
    "    classifier.train()  # set the model up for training\n",
    "if cuda:\n",
    "    classifier.cuda()\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### load data for evaluation of model (not training set) and separate weights and target\n",
    "validation_data = opt.data.get_eval_data()\n",
    "\n",
    "w_val = validation_data[:,input_size + 1]\n",
    "target_val = validation_data[:,input_size]\n",
    "target_val = Variable(torch.from_numpy(target_val).type(torch.LongTensor))\n",
    "y_val = np.transpose(validation_data)\n",
    "y_val = np.delete(y_val, [input_size, input_size + 1, input_size + 2], 0)\n",
    "y_val = np.transpose(y_val)\n",
    "val_data = Variable(torch.from_numpy(y_val).type(torch.Tensor))\n",
    "check = validation_data[:,input_size+2]\n",
    "                    \n",
    "# replace all negative weighted events with zero if the weight_cmd says to\n",
    "if opt.weight_cmd == \"no-neg\":\n",
    "    w_val[w_val < 0] = 0\n",
    "\n",
    "# remove weighting (aka, set all to 1) if weight_cmd says to\n",
    "if opt.weight_cmd == \"false\":\n",
    "    w_val = np.ones(w_val.shape)\n",
    "    \n",
    "w_val = Variable(torch.from_numpy(w_val).type(torch.FloatTensor))\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "################### Initialize (and load if needed) optimizer and loss function ################################\n",
    "################################################################################################################\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) # set up optimizer\n",
    "if opt.load_model:\n",
    "    optimizer.load_state_dict(torch.load(\"../models/three_layers/optimizer_\" + opt.model_name + \".pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none')# this is the loss function. reduction='none' makes it return a value for each input (reduce=False was deprecated)\n",
    "\n",
    "##################################################################################################\n",
    "################ initialize stuff before training ################################################\n",
    "\n",
    "small_loss = 1e20   # This is the initail loss under which we overwrite the model.\n",
    "                    # initialize with a large loss so everything is smaller than it\n",
    "\n",
    "# initialize loss arrays\n",
    "loss_val_array = np.array(())\n",
    "loss_array = np.array(())\n",
    "lva = 0  # this is the length of the loaded array\n",
    "la = 0  # this is the length of the other loaded array\n",
    "\n",
    "# load arrays and reset small_loss if loading model:\n",
    "if opt.load_model:\n",
    "    loss_val_array = np.load(\"/depot/cms/top/zhou907/ee_loss_val_array_1.npy\")\n",
    "    lva = len(loss_val_array)\n",
    "\n",
    "    loss_array = np.load(\"/depot/cms/top/zhou907/ee_loss_array_1.npy\")\n",
    "    la = len(loss_array)\n",
    "    \n",
    "    small_loss = np.min(loss_val_array)\n",
    "    \n",
    "\n",
    "batches_done = 0   # Counter for batches\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "569002.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-2eccdbfa7cae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m#-----------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mloss_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# append the training loss to the loss array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'loss' is not defined"
     ]
    }
   ],
   "source": [
    "###############################################################################################################\n",
    "# ******************************** START TRAINING LOOP *******************************************************\n",
    "###############################################################################################################\n",
    "for epoch in range(opt.n_epochs):   # Loop through all epochs\n",
    "    for i, x in enumerate(dataloader): # x is a batch and there are i batches in the epoch\n",
    "        \n",
    "        #-----------------------------\n",
    "        # Configure input\n",
    "        #----------------------------\n",
    "        variable_len = len(x[0])\n",
    "        weight = x[:, variable_len-2]\n",
    "        weight = Variable(weight.type(torch.FloatTensor))\n",
    "        target = x[:, variable_len-3]\n",
    "        target = Variable(target.type(torch.LongTensor))\n",
    "        x = np.transpose(x)\n",
    "        x = np.delete(x, [variable_len-3, variable_len-2, variable_len-1], 0)\n",
    "        x = np.transpose(x)\n",
    "        batch = Variable(x.type(torch.Tensor))   # Variable is a wrapper for the Tensor x was just made into\n",
    "                    \n",
    "        # replace all negative weighted events with zero if the weight_cmd says to\n",
    "        if opt.weight_cmd == \"no-neg\":\n",
    "            weight[weight < 0] = 0\n",
    "        \n",
    "        # remove weighting (aka, set all to 1) if weight_cmd says to\n",
    "        if opt.weight_cmd == \"false\":\n",
    "            weight = np.ones(weight.shape)\n",
    "            weight = Variable(torch.from_numpy(weight).type(torch.FloatTensor))\n",
    "\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Classifier\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer.zero_grad()   # Make gradients zero so they don't accumulate\n",
    "        \n",
    "        output = classifier(batch)   # apply nn to input   \n",
    "\n",
    "        # Calculate loss \n",
    "        print(target.shape)\n",
    "        break\n",
    "        loss_l = criterion(output, target) # loss_l is a vector with the loss for each event in the batch\n",
    "        loss = torch.dot(weight,loss_l)/(loss_l.shape[0])   # we take the dot product with the weights to calculate the final loss\n",
    "        loss.backward()   # Do back propagation \n",
    "        optimizer.step()   # Update parameters based on gradients for individuals\n",
    "        batches_done += 1  # increase the number of batches in the counter\n",
    "        \n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # Save and Draw Stuff  (if drawing is on)\n",
    "    #-----------------------------------------\n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.detach().numpy())\n",
    "        loss_array = np.append(loss_array, loss.detach().numpy()) # append the training loss to the loss array\n",
    "        \n",
    "        \n",
    "        out = classifier(val_data)   # run classifier on validation data to see how good it is\n",
    "        loss_val = torch.dot(w_val, criterion(out, target_val))/(target_val.shape[0])   # calculate the validation loss\n",
    "        loss_val_array = np.append(loss_val_array, loss_val.detach().numpy()) # append the validation loss to its array\n",
    "        \n",
    "        if small_loss > loss_val:   # compare to see if the loss has decreased\n",
    "            small_loss = loss_val   # if the network has improved replace the best loss with this one\n",
    "            torch.save(classifier.state_dict(), \"../models/five_layers/\" + opt.model_name + \".pt\")   # save the new (and better) model\n",
    "            torch.save(optimizer.state_dict(), \"../models/five_layers/optimizer_\" + opt.model_name + \".pt\") # save the optimizer state\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            # save the loss arrays\n",
    "            np.save(\"../data/five_layers/\" + opt.model_name +  \"_loss_val_array.npy\",loss_val_array)\n",
    "            np.save(\"../data/five_layers/\" + opt.model_name + \"_loss_train_array.npy\",loss_array)\n",
    "            if opt.draw:\n",
    "                #----------------------------------\n",
    "                # Draw training and validation loss\n",
    "                #-------------------------------------\n",
    "                display.clear_output(True)\n",
    "                figure = plt.figure()\n",
    "                ax = figure.add_subplot(111)\n",
    "                #ax.set_yscale(\"log\")\n",
    "                ax.plot(np.array(list(range(int((epoch)/10)+lva+1))), loss_array, label=\"training loss\")\n",
    "                ax.plot(np.array(list(range(int((epoch)/10)+la+1))), loss_val_array, label = \"evaluation loss\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: GANS_7]",
   "language": "python",
   "name": "conda-env-GANS_7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
