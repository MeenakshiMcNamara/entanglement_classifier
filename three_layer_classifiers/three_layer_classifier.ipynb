{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "######################################### Import statements ###########################################\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")   # this allows us to import from sibling directory\n",
    "\n",
    "from code_to_import.dataset_preprocessing import ProductionModeDataset\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "#import torchvision.transforms as transforms\n",
    "#from torchvision.utils import save_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F   # NOTE: I don't think this is used\n",
    "import torch.autograd as autograd\n",
    "import torch\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lb_delta_eta', 'lbbar_delta_eta', 'lnu_delta_eta', 'lnubar_delta_eta', 'lbarb_delta_eta', 'lbarbbar_delta_eta', 'lbarnu_delta_eta', 'lbarnubar_delta_eta', 'bnu_delta_eta', 'bnubar_delta_eta', 'bbarnu_delta_eta', 'bbarnubar_delta_eta', 'lb_delta_phi', 'lbbar_delta_phi', 'lnu_delta_phi', 'lnubar_delta_phi', 'lbarb_delta_phi', 'lbarbbar_delta_phi', 'lbarnu_delta_phi', 'lbarnubar_delta_phi', 'bnu_delta_phi', 'bnubar_delta_phi', 'bbarnu_delta_phi', 'bbarnubar_delta_phi', 'wplusb_delta_eta', 'wplusbbar_delta_eta', 'wminusb_delta_eta', 'wminusbbar_delta_eta', 'wplusb_delta_phi', 'wplusbbar_delta_phi', 'wminusb_delta_phi', 'wminusbbar_delta_phi', 'top_eta', 'top_boosted_eta', 'tbar_eta', 'tbar_boosted_eta', 'ttbar_delta_eta', 'ttbar_eta', 'llbar_delta_eta', 'bbbar_delta_eta', 'nunubar_delta_eta', 'top_phi', 'tbar_phi', 'ttbar_phi', 'ttbar_delta_phi', 'llbar_phi', 'llbar_delta_phi', 'bbbar_phi', 'bbbar_delta_phi', 'nunubar_phi', 'nunubar_delta_phi', 'l_eta', 'lbar_eta', 'l_phi', 'lbar_phi', 'b_eta', 'bbar_eta', 'b_phi', 'bbar_phi', 'nu_eta', 'nubar_eta', 'nu_phi', 'nubar_phi', 'wplus_eta', 'wminus_eta', 'wplus_phi', 'wminus_phi', 'top_pt', 'tbar_pt', 'l_pt', 'b_pt', 'bbar_pt', 'nu_pt', 'nubar_pt', 'met_pt', 'ttbar_pt', 'ttbar_boosted_pt', 'wplus_pt', 'wminus_pt', 'ttbar_mass', 'production_mode', 'eventWeight', '__index__']\n",
      "num qqbar = 6324\n",
      "training (15367, 83)\n",
      "evaluating (3605, 83)\n",
      "The model in this run is testthreeLayerModel_ee_corrCut_-1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "######################## THIS CLASS INCLUDES ALL THE VARIABLES YOU WANT TO CONFIGURE #################################\n",
    "#######################################################################################################################\n",
    "\n",
    "class opt():   # Class used for optimizers in the future. Defines all variables and stuff needed.\n",
    "#     save_weights = True  # Tells whether to save weights... currently not used\n",
    "    \n",
    "    load_model = False  # set true if you want to load and continue training a previous model\n",
    "    \n",
    "    draw = True # set to false when running on slurm\n",
    "    \n",
    "    n_epochs = 40000   # an epoch is the number of times it works through the entire training set.\n",
    "                       # This sets the total number of epochs which will be run\n",
    "    \n",
    "    batch_size = 5000   # the training set is broken up into batches, \n",
    "                        # this sets the size of each batch\n",
    "    \n",
    "    lr = 0.001   # learning rate (how much to change based on error)\n",
    "    b1 = 0.9   # Used for Adam. Exponential decay rate for the first moment\n",
    "    b2 = 0.999   # Used for Adam. Exponential decay rate for the second moment estimates (gradient squared)\n",
    "        \n",
    "    correlation_cut = -1  # this is the correlation cut... If negative then no cut is applied\n",
    "    \n",
    "    # the root_path leads to the folder with the root files being used for data\n",
    "    root_path = \"/depot-new/cms/top/mcnama20/TopSpinCorr-Run2-Entanglement/CMSSW_10_2_22/src/TopAnalysis/Configuration/analysis/diLeptonic/three_files/Nominal\"\n",
    "\n",
    "    file = root_path + \"/ee_modified_root_1.root\"   # this is the data root file loaded into the dataloader\n",
    "    \n",
    "    model_name = \"testthreeLayerModel_ee_corrCut_\" + str(correlation_cut) # this is the model name. \n",
    "                                                                          # Change it when running a new model\n",
    "    \n",
    "    # load data object so we can access validation and training data\n",
    "    if correlation_cut > 0:\n",
    "        data = ProductionModeDataset(file, correlation_cut=correlation_cut)\n",
    "    else:\n",
    "        data = ProductionModeDataset(file)\n",
    "\n",
    "print(\"The model in this run is \" + opt.model_name)   # this will make slurm output easier to identify\n",
    "\n",
    "\n",
    "cuda = True if torch.cuda.is_available() else False\n",
    "\n",
    "\n",
    "############################## Configure data loader depending on if there is a correlation cut ###########################\n",
    "if opt.correlation_cut > 0:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        opt.data,\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "else:\n",
    "    os.makedirs(\"../data/three_layers/\", exist_ok=True)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        opt.data,\n",
    "        batch_size=opt.batch_size, drop_last=True,\n",
    "        shuffle=True,\n",
    "    )\n",
    "print('done')\n",
    "\n",
    "data = iter(dataloader)\n",
    "x = data.next()\n",
    "input_size = x.shape[1] -3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###################### Now save training and validation sets for future analysis ###################################\n",
    "np.save(\"../data/three_layers/train_val_datasets/training_dataset_\" + opt.model_name + \".npy\", opt.data.get_training_data())\n",
    "np.save(\"../data/three_layers/train_val_datasets/validation_dataset_\" + opt.model_name + \".npy\", opt.data.get_eval_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    \"\"\"\n",
    "    classifier layers\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()   # Just uses the module constructor with name Discriminator \n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_size, 512),   # first layer\n",
    "            nn.LeakyReLU(0.2, inplace=True),   # apply leaky relu to layer\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 3),\n",
    "            nn.LeakyReLU(0.2, inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        applies model to input and attempts to classify\n",
    "        \"\"\"\n",
    "        output = self.model(input)   # Classifies the input (at location) as gg (0) qqbar (1) or other (2)\n",
    "        return output\n",
    "\n",
    "\n",
    "# ******* OUT OF CLASSES NOW ************\n",
    "\n",
    "############### Initialize classifier and load a model if needed ##########################\n",
    "classifier = Classifier()\n",
    "if opt.load_model:\n",
    "    classifier.load_state_dict(torch.load(\"../models/three_layers/\" + opt.model_name + \".pt\")) #load module with same name\n",
    "    classifier.train()  # set the model up for training\n",
    "if cuda:\n",
    "    classifier.cuda()\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####################################### load data for evaluation of model (not training set) and separate weights and target\n",
    "validation_data = opt.data.get_eval_data()\n",
    "\n",
    "w_val = validation_data[:,input_size + 1]\n",
    "w_val = Variable(torch.from_numpy(w_val).type(torch.FloatTensor))\n",
    "target_val = validation_data[:,input_size]\n",
    "target_val = Variable(torch.from_numpy(target_val).type(torch.LongTensor))\n",
    "y_val = np.transpose(validation_data)\n",
    "y_val = np.delete(y_val, [input_size, input_size + 1, input_size + 2], 0)\n",
    "y_val = np.transpose(y_val)\n",
    "val_data = Variable(torch.from_numpy(y_val).type(torch.Tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3391.459\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3baa005335d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m###############################################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m   \u001b[0;31m# Loop through all epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# x is a batch and there are i batches in the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#-----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/cent7/5.1.0-py36/GANS_7/lib/python3.6/site-packages/torch/utils/data/_utils/collate.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_collate_err_msg_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mdefault_collate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0melem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# scalars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "################### Initialize (and load if needed) optimizer and loss function ################################\n",
    "################################################################################################################\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=opt.lr, betas=(opt.b1, opt.b2)) # set up optimizer\n",
    "if opt.load_model:\n",
    "    optimizer.load_state_dict(torch.load(\"../models/three_layers/optimizer_\" + opt.model_name + \".pt\"))\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduce=False)# this is the loss function. reduce=false makes it return a value for each input\n",
    "\n",
    "##################################################################################################\n",
    "################ initialize stuff before training ################################################\n",
    "\n",
    "small_loss = 1e20   # This is the initail loss under which we overwrite the model.\n",
    "                    # initialize with a large loss so everything is smaller than it\n",
    "\n",
    "# initialize loss arrays\n",
    "loss_val_array = np.array(())\n",
    "loss_array = np.array(())\n",
    "lva = 0  # this is the length of the loaded array\n",
    "la = 0  # this is the length of the other loaded array\n",
    "\n",
    "# load arrays and reset small_loss if loading model:\n",
    "if opt.load_model:\n",
    "    loss_val_array = np.load(\"../data/three_layers/\" + opt.model_name +  \"_loss_val_array.npy\")\n",
    "    lva = len(loss_val_array)\n",
    "\n",
    "    loss_array = np.load(\"../data/three_layers/\" + opt.model_name +  \"_loss_train_array.npy\")\n",
    "    la = len(loss_array)\n",
    "    \n",
    "    small_loss = np.min(loss_val_array)\n",
    "    \n",
    "\n",
    "batches_done = 0   # Counter for batches\n",
    "\n",
    "\n",
    "###############################################################################################################\n",
    "# ******************************** START TRAINING LOOP *******************************************************\n",
    "###############################################################################################################\n",
    "for epoch in range(opt.n_epochs):   # Loop through all epochs\n",
    "    for i, x in enumerate(dataloader): # x is a batch and there are i batches in the epoch\n",
    "        \n",
    "        #-----------------------------\n",
    "        # Configure input\n",
    "        #----------------------------\n",
    "        variable_len = len(x[0])\n",
    "        weight = x[:, variable_len-2]\n",
    "        weight = Variable(weight.type(torch.FloatTensor))\n",
    "        target = x[:, variable_len-3]\n",
    "        target = Variable(target.type(torch.LongTensor))\n",
    "        x = np.transpose(x)\n",
    "        x = np.delete(x, [variable_len-3, variable_len-2, variable_len-1], 0)\n",
    "        x = np.transpose(x)\n",
    "        batch = Variable(x.type(torch.Tensor))   # Variable is a wrapper for the Tensor x was just made into\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Classifier\n",
    "        # ---------------------\n",
    "\n",
    "        optimizer.zero_grad()   # Make gradients zero so they don't accumulate\n",
    "        \n",
    "        output = classifier(batch)   # apply nn to input   \n",
    "\n",
    "        # Calculate loss \n",
    "        loss_l = criterion(output, target) # loss_l is a vector with the loss for each event in the batch\n",
    "        loss = torch.dot(weight,loss_l)   # we take the dot product with the weights to calculate the final loss\n",
    "        loss.backward()   # Do back propagation \n",
    "        optimizer.step()   # Update parameters based on gradients for individuals\n",
    "        batches_done += 1  # increase the number of batches in the counter\n",
    "        \n",
    "    \n",
    "    #-----------------------------------------\n",
    "    # Save and Draw Stuff  (if drawing is on)\n",
    "    #-----------------------------------------\n",
    "    if epoch % 10 == 0:\n",
    "        print(loss.detach().numpy())\n",
    "        loss_array = np.append(loss_array, loss.detach().numpy()) # append the training loss to the loss array\n",
    "        \n",
    "        \n",
    "        out = classifier(val_data)   # run classifier on validation data to see how good it is\n",
    "        loss_val = torch.dot(w_val, criterion(out, target_val))   # calculate the validation loss\n",
    "        loss_val_array = np.append(loss_val_array, loss_val.detach().numpy()) # append the validation loss to its array\n",
    "        \n",
    "        if small_loss > loss_val:   # compare to see if the loss has decreased\n",
    "            small_loss = loss_val   # if the network has improved replace the best loss with this one\n",
    "            torch.save(classifier.state_dict(), \"../models/three_layers/\" + opt.model_name + \".pt\")   # save the new (and better) model\n",
    "            torch.save(optimizer.state_dict(), \"../models/three_layers/optimizer_\" + opt.model_name + \".pt\") # save the optimizer state\n",
    "            \n",
    "        if epoch % 50 == 0:\n",
    "            # save the loss arrays\n",
    "            np.save(\"../data/three_layers/\" + opt.model_name +  \"_loss_val_array.npy\",loss_val_array)\n",
    "            np.save(\"../data/three_layers/\" + opt.model_name + \"_loss_train_array.npy\",loss_array)\n",
    "            if opt.draw:\n",
    "                #----------------------------------\n",
    "                # Draw training and validation loss\n",
    "                #-------------------------------------\n",
    "                display.clear_output(True)\n",
    "                figure = plt.figure()\n",
    "                ax = figure.add_subplot(111)\n",
    "                #ax.set_yscale(\"log\")\n",
    "                ax.plot(np.array(list(range(int((epoch)/10)+lva+1))), loss_array, label=\"training loss\")\n",
    "                ax.plot(np.array(list(range(int((epoch)/10)+la+1))), loss_val_array, label = \"evaluation loss\")\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env: GANS_7]",
   "language": "python",
   "name": "conda-env-GANS_7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
